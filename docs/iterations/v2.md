# LLM Cloud-Agnostic K8s Terraform — V2: Interactive Streamlit & HuggingFace Deployment

Cloud-agnostic, Terraform-driven deployment of an **interactive Streamlit app** for LLM browsing, selection, and live inference on Kubernetes (GKE, EKS, or AKS) with HuggingFace integration.

## Overview

This V2 project empowers users to spin up a fully automated, cloud-agnostic MLOps stack featuring:

- **A Streamlit-powered web interface** for interactive browsing of public language models directly from HuggingFace.
- **One-click cloud-native deployment**: Instantly launch both the Streamlit app and dynamic LLM-serving endpoints on your cloud provider (GCP, AWS, or Azure).
- **User-driven LLM selection and inference**: After deployment, users can browse available models, select one, and deploy it for live API-based inference—all in their browser.
- **Multiple, independent sessions/tabs**: Launch, manage, and detach as many model deployments as resource limits allow.

## Features

- **Cloud-agnostic IaC:** Deploy on Google Kubernetes Engine (GKE), Amazon EKS, or Azure AKS using unified Terraform.
- **Self-serve Streamlit UI:** Query and select any open-access LLM from HuggingFace in real time.
- **Zero local setup for inference:** All inference happens via the browser—no CLI, no code.
- **Dynamic model serving:** Deploy and serve LLMs on demand, with endpoint status and results tracked in the app.
- **Multi-model & multi-session support:** Open several browser tabs and independently deploy/query multiple language models.
- **Easy teardown and cleanup:** Destroy resources (and costs) with a single script.

## Quick Start

### Prerequisites

- GitHub account
- Terraform CLI and cloud CLI (`gcloud`, `aws`, or `az`)
- Access to GKE, EKS, or AKS (cloud account with permissions, billing enabled)
- `kubectl`
- Sufficient cloud quota (CPU, GPU if needed, networking, etc.)

### Deployment Steps

1. **Clone the Repository**

   ```bash
   git clone https://github.com/sanskargupta551/llm-cloud-agnostic-k8s-terraform-v2.git
   cd llm-cloud-agnostic-k8s-terraform-v2
   ```

2. **Configure Terraform for Your Cloud**

   - Edit files in `terraform/` (e.g., `main.tf`, `variables.tf`) to pick your cloud provider and region.
   - Set your cloud credentials as environment variables or provider config.

3. **Provision Infrastructure**

   ```bash
   terraform init
   terraform plan
   terraform apply
   ```

4. **Launch the Streamlit App**

   - When deployment finishes, the Streamlit UI endpoint will be shown in Terraform’s outputs or the deploy log.
   - Open the provided URL in your browser.

5. **Browse and Deploy LLMs**

   - Use the integrated Streamlit page to **browse HuggingFace public LLMs**.
   - Select a model and click **“Deploy”**. The app will use your cluster resources to stand up a live inference endpoint for the chosen model.
   - **Run inference instantly:** Input a prompt/query and review the model responses directly in the interface.
   - **Repeat in multiple tabs** if you want several models running at once, up to your account’s resource limits.

6. **Teardown**

   To remove all cloud resources:
   ```bash
   ./scripts/destroy.sh
   ```
---

## Directory Structure

```
llm-cloud-agnostic-k8s-terraform-v2/
│
├── README.md
├── LICENSE
├── .gitignore
│
├── terraform/                  # Cloud-agnostic Terraform IaC
│   ├── main.tf
│   ├── variables.tf
│   ├── outputs.tf
│   ├── providers.tf
│   ├── versions.tf
│   ├── modules/
│   └── environments/
│
├── k8s/                        # Kubernetes manifests
│   ├── llm-deployment.yaml     # LLM model serving deployment template
│   ├── llm-service.yaml
│   ├── streamlit.yaml          # Streamlit webapp deployment
│   ├── streamlit-service.yaml
│   ├── configmap.yaml
│   └── hpa.yaml
│
├── streamlit_app/              # Streamlit source code
│   ├── app.py
│   ├── requirements.txt
│   ├── huggingface_helpers.py
│   ├── utils.py
│   └── assets/
│
├── scripts/                    # Automation scripts
│   ├── deploy.sh
│   ├── destroy.sh
│   └── validate.sh
│
├── docs/
└── assets/
```

## Supported Models

- **Browse and select** from all HuggingFace public text-generation LLMs
- **Default example:** Llama-2 7B ([fine-tuning and extension encouraged][1])
- Compatible with any open-weight Transformers-based model

## License

This project is licensed under the **MIT License**.  
You are free to use, modify, and distribute with attribution.

## Contributing

Contributions, issues, and discussions are welcome! Please open an issue or submit a pull request for suggestions or improvements.

## Acknowledgements

- Meta AI (Llama-2), HuggingFace, Streamlit
- Terraform, Kubernetes, and cloud providers GCP/AWS/Azure
- The open-source MLOps, LLM, and infra communities

*Launch, explore, and inference with state-of-the-art LLMs—cloud-native, portable, and all through a powerful, easy-to-use web UI.*

[1]: Interested in fine-tuning Large Language Models like Llama-2 7B for specific applications (skills.ai_development, projects.documentation)

[1] skills.ai_development